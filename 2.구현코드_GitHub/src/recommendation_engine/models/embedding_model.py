#!/usr/bin/env python3
"""
HyurimBot í•œêµ­ì–´ BERT ì„ë² ë”© ì‹œìŠ¤í…œ - ê¸°íšì•ˆ ëª©ì  ì™„ì „ êµ¬í˜„
RAG ê¸°ë°˜ ìì—°íœ´ì–‘ë¦¼ AI ì¶”ì²œì„ ìœ„í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ ì—”ì§„

âœ… ê¸°íšì•ˆ í•µì‹¬ ìš”êµ¬ì‚¬í•­ êµ¬í˜„:
- ê°œì¸í™” ë§ì¶¤ ì¶”ì²œ: ì‚¬ìš©ì ì…ë ¥(ê°€ì¡±êµ¬ì„±, ì§€ì—­, í…Œë§ˆ) ê¸°ë°˜ ë²¡í„°í™”
- í’ë¶€í•œ ì •ë³´ ì œê³µ: ì‹œì„¤ëª…, í¸ì˜ì‹œì„¤, ê°€ê²©, ìœ„ì¹˜ í†µí•© ì„ë² ë”©  
- í•œêµ­ì–´ íŠ¹í™”: jhgan/ko-sroberta-multitask ëª¨ë¸ í™œìš©
- ì‹¤ì‹œê°„ ê²€ìƒ‰: FAISS ì—°ë™ì„ ìœ„í•œ ì •ê·œí™”ëœ ë²¡í„° ì¶œë ¥
"""

import os
import logging
import numpy as np
import sqlite3
import pickle
import re
from typing import List, Dict, Optional, Union, Tuple
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HyurimBotEmbeddingEngine:
    """
    HyurimBot ì „ìš© í•œêµ­ì–´ BERT ì„ë² ë”© ì—”ì§„
    ê¸°íšì•ˆì˜ RAG ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬í˜„
    """
    
    def __init__(self, db_path: str, model_name: str = "jhgan/ko-sroberta-multitask"):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ 
            model_name: ê¸°íšì•ˆ ì§€ì • í•œêµ­ì–´ BERT ëª¨ë¸
        """
        self.db_path = db_path
        self.model_name = model_name
        self.model = None
        self.accommodations_data = []
        self.embeddings = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.cache_dir = Path(db_path).parent.parent / "data" / "embeddings"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.embeddings_cache_path = self.cache_dir / "hyurimbot_embeddings.pkl"
        
        logger.info(f"ğŸ¤– HyurimBot ì„ë² ë”© ì—”ì§„ ì´ˆê¸°í™”")
        logger.info(f"ğŸ“ ìºì‹œ ê²½ë¡œ: {self.cache_dir}")
        
        # ìë™ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì™€ ëª¨ë¸ ë¡œë“œ
        self._load_accommodation_data()
        self._initialize_model()
    
    def _initialize_model(self):
        """ê¸°íšì•ˆ ì§€ì • í•œêµ­ì–´ BERT ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # SentenceTransformer ì„í¬íŠ¸ (ì„ íƒì )
            try:
                from sentence_transformers import SentenceTransformer
                logger.info(f"ğŸš€ í•œêµ­ì–´ BERT ëª¨ë¸ ë¡œë”©: {self.model_name}")
                self.model = SentenceTransformer(self.model_name)
                logger.info("âœ… í•œêµ­ì–´ BERT ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                return
            except ImportError:
                logger.warning("âš ï¸ sentence-transformers ë¯¸ì„¤ì¹˜, ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©")
            except Exception as e:
                logger.warning(f"âš ï¸ BERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©: {e}")
            
            # ê¸°ë³¸ ì„ë² ë”© (TF-IDF ê¸°ë°˜) ì‚¬ìš©
            self.model = "basic_tfidf"
            logger.info("âœ… ê¸°ë³¸ TF-IDF ì„ë² ë”© ì‹œìŠ¤í…œ ì‚¬ìš©")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _load_accommodation_data(self):
        """ê¸°íšì•ˆì— ë§ëŠ” í’ë¶€í•œ ìˆ™ë°•ì‹œì„¤ ë°ì´í„° ë¡œë“œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # ê¸°íšì•ˆ ìš”êµ¬: ì‹œì„¤, ê°€ê²©, ì •ì±…, í¸ì˜ì‹œì„¤ ëª¨ë“  ì •ë³´ í¬í•¨
            cursor.execute("""
                SELECT 
                    a.accommodation_id,
                    a.forest_id,
                    a.facility_name,
                    a.facility_type,
                    a.capacity_standard,
                    a.capacity_maximum,
                    a.area,
                    a.amenities,
                    a.price_off_weekday,
                    a.price_peak_weekend,
                    a.usage_info,
                    f.forest_name,
                    f.sido,
                    f.main_facilities,
                    f.address,
                    f.phone,
                    f.homepage_url
                FROM accommodations a
                JOIN forests f ON a.forest_id = f.forest_id
                WHERE a.facility_name IS NOT NULL 
                  AND a.facility_name != ''
                ORDER BY f.forest_name, a.facility_name
            """)
            
            rows = cursor.fetchall()
            self.accommodations_data = [dict(row) for row in rows]
            conn.close()
            
            logger.info(f"ğŸ“¦ {len(self.accommodations_data)}ê°œ ìˆ™ë°•ì‹œì„¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
            # ìƒ˜í”Œ ë°ì´í„° ë¡œê·¸
            if self.accommodations_data:
                sample = self.accommodations_data[0]
                logger.info(f"ğŸ“‹ ìƒ˜í”Œ: {sample.get('facility_name')} ({sample.get('forest_name')})")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.accommodations_data = []
    
    def _create_rich_feature_text(self, accommodation: Dict) -> str:
        """
        ê¸°íšì•ˆ ìš”êµ¬: í’ë¶€í•œ íŠ¹ì„± í…ìŠ¤íŠ¸ ìƒì„±
        ì‹œì„¤ëª…, í¸ì˜ì‹œì„¤, ì§€ì—­, ê°€ê²©, ê°€ì¡±êµ¬ì„± ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ë°˜ì˜
        """
        features = []
        
        # 1. ê¸°ë³¸ ì‹œì„¤ ì •ë³´ (ê°€ì¥ ì¤‘ìš”)
        if accommodation.get('facility_name'):
            features.append(f"ì‹œì„¤: {accommodation['facility_name']}")
        
        if accommodation.get('facility_type'):
            features.append(f"ìœ í˜•: {accommodation['facility_type']}")
        
        # 2. ìœ„ì¹˜ ì •ë³´ (ê¸°íšì•ˆ: ì§€ì—­ë³„ ì¶”ì²œ)
        if accommodation.get('forest_name'):
            features.append(f"íœ´ì–‘ë¦¼: {accommodation['forest_name']}")
        
        if accommodation.get('sido'):
            features.append(f"ì§€ì—­: {accommodation['sido']}")
        
        # 3. ê°€ì¡±êµ¬ì„± ê¸°ë°˜ ìˆ˜ìš©ì¸ì› (ê¸°íšì•ˆ í•µì‹¬)
        capacity = accommodation.get('capacity_standard')
        if capacity:
            if capacity <= 4:
                features.append("ì†Œê·œëª¨ê°€ì¡± ì»¤í”Œì—¬í–‰ 2~4ì¸")
            elif capacity <= 8:
                features.append("ì¤‘ê°„ê°€ì¡± ê°€ì¡±ì—¬í–‰ 4~8ì¸")
            else:
                features.append("ëŒ€ê°€ì¡± ë‹¨ì²´ì—¬í–‰ 8ì¸ì´ìƒ")
                
            features.append(f"ìˆ˜ìš©ì¸ì›: {capacity}ëª…")
        
        # 4. í¸ì˜ì‹œì„¤ (ê¸°íšì•ˆ: ì¤‘ìš” íŠ¹ì„±)
        amenities_text = []
        if accommodation.get('amenities'):
            amenities = accommodation['amenities'].replace(';', ' ')
            amenities_text.append(f"í¸ì˜ì‹œì„¤: {amenities}")
                
        features.extend(amenities_text)
        
        # 5. ê°€ê²©ëŒ€ ì •ë³´ (ê¸°íšì•ˆ: ì²´ê°ê°€ ì¤‘ìš”)
        prices = []
        if accommodation.get('price_off_weekday'):
            prices.append(accommodation['price_off_weekday'])
        if accommodation.get('price_peak_weekend'):
            prices.append(accommodation['price_peak_weekend'])
        
        if prices:
            avg_price = sum(prices) // len(prices)
            if avg_price < 100000:
                price_category = "ì €ê°€ê²©ëŒ€ ê²½ì œì "
            elif avg_price < 200000:
                price_category = "ì¤‘ê°€ê²©ëŒ€ í•©ë¦¬ì "
            else:
                price_category = "ê³ ê°€ê²©ëŒ€ í”„ë¦¬ë¯¸ì—„"
            
            features.append(f"ê°€ê²©: {price_category}")
            features.append(f"í‰ê· ìš”ê¸ˆ: {avg_price:,}ì›")
        
        # 6. íœ´ì–‘ë¦¼ ë¶€ëŒ€ì‹œì„¤
        if accommodation.get('main_facilities'):
            facilities = accommodation['main_facilities'][:100]
            features.append(f"ë¶€ëŒ€ì‹œì„¤: {facilities}")
        
        # 7. ì´ìš© íŠ¹ì„±
        if accommodation.get('usage_info'):
            usage_info = accommodation['usage_info'][:100]
            features.append(f"ì´ìš©ì•ˆë‚´: {usage_info}")
        
        return " | ".join(features)
    
    def generate_embeddings(self, force_regenerate: bool = False) -> np.ndarray:
        """
        ê¸°íšì•ˆ ëª©ì : ëª¨ë“  ìˆ™ë°•ì‹œì„¤ì— ëŒ€í•œ ì˜ë¯¸ì  ì„ë² ë”© ìƒì„±
        
        Args:
            force_regenerate: ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±í• ì§€ ì—¬ë¶€
        
        Returns:
            ì •ê·œí™”ëœ ì„ë² ë”© í–‰ë ¬ (N x embedding_dim)
        """
        # ìºì‹œ í™•ì¸
        if not force_regenerate and os.path.exists(self.embeddings_cache_path):
            try:
                logger.info("ğŸ“‹ ìºì‹œëœ ì„ë² ë”© ë¡œë”©...")
                with open(self.embeddings_cache_path, 'rb') as f:
                    cached_data = pickle.load(f)
                    self.embeddings = cached_data['embeddings']
                    logger.info(f"âœ… ìºì‹œ ì„ë² ë”© ë¡œë”© ì™„ë£Œ: {self.embeddings.shape}")
                    return self.embeddings
            except Exception as e:
                logger.warning(f"âš ï¸ ìºì‹œ ë¡œë”© ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±: {e}")
        
        if not self.accommodations_data:
            logger.error("âŒ ìˆ™ë°•ì‹œì„¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return np.array([])
        
        logger.info("ğŸš€ ìˆ™ë°•ì‹œì„¤ ì„ë² ë”© ìƒì„± ì‹œì‘...")
        
        # í’ë¶€í•œ íŠ¹ì„± í…ìŠ¤íŠ¸ ìƒì„±
        feature_texts = []
        for accommodation in self.accommodations_data:
            feature_text = self._create_rich_feature_text(accommodation)
            feature_texts.append(feature_text)
        
        # BERT ë˜ëŠ” TF-IDF ì„ë² ë”© ìƒì„±
        try:
            if isinstance(self.model, str) and self.model == "basic_tfidf":
                # TF-IDF ê¸°ë°˜ ì„ë² ë”©
                self.embeddings = self._generate_tfidf_embeddings(feature_texts)
            else:
                # BERT ì„ë² ë”©
                self.embeddings = self.model.encode(
                    feature_texts,
                    batch_size=16,
                    show_progress_bar=True,
                    normalize_embeddings=True
                )
            
            # ìºì‹œ ì €ì¥
            cache_data = {
                'embeddings': self.embeddings,
                'accommodation_ids': [acc['accommodation_id'] for acc in self.accommodations_data],
                'model_name': self.model_name,
                'feature_count': len(self.accommodations_data)
            }
            
            with open(self.embeddings_cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.embeddings.shape}")
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {self.embeddings_cache_path}")
            
            return self.embeddings
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return np.array([])
    
    def _generate_tfidf_embeddings(self, texts: List[str]) -> np.ndarray:
        """TF-IDF ê¸°ë°˜ ê¸°ë³¸ ì„ë² ë”© ìƒì„± (BERT ëŒ€ì²´ìš©)"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.preprocessing import normalize
        
        # í•œêµ­ì–´ íŠ¹í™” TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            stop_words=None,  # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë³„ë„ ì²˜ë¦¬ í•„ìš”
            lowercase=False  # í•œêµ­ì–´ëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆí•¨
        )
        
        # TF-IDF ë²¡í„° ìƒì„±
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # ì •ê·œí™”ëœ dense ë°°ì—´ë¡œ ë³€í™˜
        embeddings = normalize(tfidf_matrix.toarray(), norm='l2')
        
        logger.info(f"ğŸ”§ TF-IDF ì„ë² ë”© ìƒì„±: {embeddings.shape}")
        return embeddings
    
    def encode_user_query(self, query: str) -> np.ndarray:
        """
        ê¸°íšì•ˆ ëª©ì : ì‚¬ìš©ì ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        ê°œì¸í™” ìš”ì†Œ(ê°€ì¡±êµ¬ì„±, ì§€ì—­, í…Œë§ˆ)ë¥¼ ìë™ ì¶”ì¶œí•˜ì—¬ ë°˜ì˜
        
        Args:
            query: ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "4ì¸ ê°€ì¡± ë„“ì€ ê°ì‹¤")
        
        Returns:
            ì •ê·œí™”ëœ ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        """
        if not query:
            logger.warning("âŒ ë¹ˆ ì¿¼ë¦¬ì…ë‹ˆë‹¤")
            return np.array([])
        
        try:
            # ê¸°íšì•ˆ ìš”êµ¬: ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° ë³´ê°•
            enhanced_query = self._enhance_user_query(query)
            
            if isinstance(self.model, str) and self.model == "basic_tfidf":
                # TF-IDFë¡œ ì¿¼ë¦¬ ì„ë² ë”©
                return self._encode_query_tfidf(enhanced_query)
            else:
                # BERTë¡œ ì¿¼ë¦¬ ì„ë² ë”©
                query_embedding = self.model.encode(
                    [enhanced_query],
                    normalize_embeddings=True
                )
                return query_embedding[0]
                
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.array([])
    
    def _enhance_user_query(self, query: str) -> str:
        """
        ê¸°íšì•ˆ ìš”êµ¬: ì‚¬ìš©ì ì¿¼ë¦¬ ì˜ë„ íŒŒì•… ë° ë³´ê°•
        ê°€ì¡±êµ¬ì„±, ì§€ì—­, í…Œë§ˆ ì •ë³´ë¥¼ ìë™ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
        """
        enhanced = query.strip()
        
        # 1. ì¸ì› ì •ë³´ ì¶”ì¶œ (ê¸°íšì•ˆ: ê°€ì¡±êµ¬ì„± ê¸°ë°˜)
        capacity_match = re.search(r'(\d+)(?:ì¸|ëª…|ì¸ìš©)', enhanced)
        if capacity_match:
            capacity = int(capacity_match.group(1))
            if capacity <= 4:
                enhanced += " ì†Œê·œëª¨ê°€ì¡± ì»¤í”Œì—¬í–‰"
            elif capacity <= 8:
                enhanced += " ì¤‘ê°„ê°€ì¡± ê°€ì¡±ì—¬í–‰"
            else:
                enhanced += " ëŒ€ê°€ì¡± ë‹¨ì²´ì—¬í–‰"
        
        # 2. ê°€ì¡± ê´€ë ¨ í‚¤ì›Œë“œ ë³´ê°•
        family_keywords = ['ê°€ì¡±', 'ì•„ì´', 'ì–´ë¦°ì´', 'ë¶€ëª¨', 'í• ë¨¸ë‹ˆ', 'í• ì•„ë²„ì§€', 'ìë…€']
        if any(keyword in enhanced for keyword in family_keywords):
            enhanced += " ê°€ì¡±ì¹œí™”ì  í¸ì˜ì‹œì„¤"
        
        # 3. í¸ì˜ì‹œì„¤ í‚¤ì›Œë“œ ë³´ê°•
        amenity_keywords = ['ë„“ì€', 'ê¹¨ë—í•œ', 'ì¡°ìš©í•œ', 'í¸ë¦¬í•œ', 'ì „ë§', 'ë·°']
        if any(keyword in enhanced for keyword in amenity_keywords):
            enhanced += " ê³ ê¸‰í¸ì˜ì‹œì„¤ ì¾Œì í•œ"
        
        # 4. ì§€ì—­ í‚¤ì›Œë“œ ê°ì§€
        region_keywords = ['ì œì£¼', 'ê°•ì›', 'ê²½ê¸°', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨']
        found_regions = [region for region in region_keywords if region in enhanced]
        if found_regions:
            enhanced += f" {' '.join(found_regions)}ì§€ì—­"
        
        # 5. í…Œë§ˆ í‚¤ì›Œë“œ ë³´ê°• (ê¸°íšì•ˆ: í…Œë§ˆë³„ ì¶”ì²œ)
        theme_keywords = {
            'íë§': 'ì¡°ìš©í•œ íœ´ì‹ ì‚°ë¦¼ìš•',
            'ì•¡í‹°ë¹„í‹°': 'ì²´í—˜í”„ë¡œê·¸ë¨ ë ˆí¬ì¸ ',
            'ìì—°': 'ì‚°ë¦¼ìš• ìì—°íœ´ì–‘',
            'ì „í†µ': 'ì´ˆê°€ì§‘ í•œì˜¥ ì „í†µ',
            'í”„ë¦¬ë¯¸ì—„': 'ê³ ê¸‰ ëŸ­ì…”ë¦¬ íŠ¹ê¸‰'
        }
        
        for theme, enhancement in theme_keywords.items():
            if theme in enhanced:
                enhanced += f" {enhancement}"
        
        logger.info(f"ğŸ” ì¿¼ë¦¬ ë³´ê°•: '{query}' â†’ '{enhanced}'")
        return enhanced
    
    def _encode_query_tfidf(self, query: str) -> np.ndarray:
        """TF-IDF ë°©ì‹ìœ¼ë¡œ ì¿¼ë¦¬ ì„ë² ë”©"""
        # ê°„ë‹¨í•œ ë‹¨ì–´ ë§¤ì¹­ ê¸°ë°˜ ì„ë² ë”©
        query_words = set(query.lower().split())
        
        if not self.accommodations_data or self.embeddings is None:
            return np.array([])
        
        # ê° ìˆ™ë°•ì‹œì„¤ê³¼ì˜ ë‹¨ì–´ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        similarity_scores = []
        for accommodation in self.accommodations_data:
            acc_text = self._create_rich_feature_text(accommodation).lower()
            acc_words = set(acc_text.split())
            
            # Jaccard ìœ ì‚¬ë„
            intersection = query_words.intersection(acc_words)
            union = query_words.union(acc_words)
            
            similarity = len(intersection) / len(union) if union else 0.0
            similarity_scores.append(similarity)
        
        # ì •ê·œí™”
        scores_array = np.array(similarity_scores)
        if scores_array.sum() > 0:
            scores_array = scores_array / np.linalg.norm(scores_array)
        
        return scores_array
    
    def get_accommodation_by_index(self, index: int) -> Optional[Dict]:
        """ì¸ë±ìŠ¤ë¡œ ìˆ™ë°•ì‹œì„¤ ì •ë³´ ë°˜í™˜"""
        if 0 <= index < len(self.accommodations_data):
            return self.accommodations_data[index]
        return None
    
    def get_embedding_info(self) -> Dict:
        """ì„ë² ë”© ì—”ì§„ ì •ë³´ ë°˜í™˜"""
        return {
            'model_name': self.model_name,
            'accommodations_count': len(self.accommodations_data),
            'embeddings_shape': self.embeddings.shape if self.embeddings is not None else None,
            'cache_path': str(self.embeddings_cache_path),
            'cache_exists': os.path.exists(self.embeddings_cache_path)
        }


def test_embedding_engine():
    """ì„ë² ë”© ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    import sys
    from pathlib import Path
    
    # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
    project_root = Path(__file__).parent.parent.parent
    db_path = project_root / "database" / "hyurimbot.db"
    
    if not db_path.exists():
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")
        return False
    
    try:
        print("ğŸ¤– HyurimBot ì„ë² ë”© ì—”ì§„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # ì—”ì§„ ì´ˆê¸°í™”
        engine = HyurimBotEmbeddingEngine(str(db_path))
        
        # ì„ë² ë”© ìƒì„±
        embeddings = engine.generate_embeddings()
        
        if embeddings.size == 0:
            print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì„±ê³µ: {embeddings.shape}")
        
        # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        test_queries = [
            "4ì¸ ê°€ì¡± ë„“ì€ ê°ì‹¤",
            "ì¡°ìš©í•œ íœ´ì–‘ë¦¼ì—ì„œ íë§",
            "ì œì£¼ë„ ì „í†µ ì´ˆê°€ì§‘",
            "ì•„ì´ì™€ í•¨ê»˜ í¸ì˜ì‹œì„¤ ì¢‹ì€ ê³³"
        ]
        
        for query in test_queries:
            query_embedding = engine.encode_user_query(query)
            if query_embedding.size > 0:
                print(f"âœ… ì¿¼ë¦¬ '{query}' ì„ë² ë”© ì„±ê³µ")
            else:
                print(f"âŒ ì¿¼ë¦¬ '{query}' ì„ë² ë”© ì‹¤íŒ¨")
        
        # ì—”ì§„ ì •ë³´
        info = engine.get_embedding_info()
        print("\nğŸ“Š ì„ë² ë”© ì—”ì§„ ì •ë³´:")
        for key, value in info.items():
            print(f"  {key}: {value}")
        
        print("\nğŸ‰ HyurimBot ì„ë² ë”© ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    success = test_embedding_engine()
    sys.exit(0 if success else 1)